<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8" />
    <!-- <meta name="referrer" content="never"> -->
    <title>Yuezun Li's Homepage</title>
    <link
      rel="stylesheet"
      href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css"
    />

    <meta name="author" content="Yuezun Li" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link href="css/bootstrap.min.css" rel="stylesheet" />
    <link href="css/bootstrap1.min.css" rel="stylesheet" />
    <link href="css/bootstrap-responsive.min.css" rel="stylesheet" />
    <link href="css/theme.css" rel="stylesheet" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"
    />
    <link rel="icon" type="image/jpg" href="images/src_img/icon.jpg" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    />
    <link rel="icon" type="image/png" href="src_img/logo.png" />
    <link
      href="https://fonts.googleapis.com/css2?family=Orbitron:wght@700&display=swap"
      rel="stylesheet"
    />
    <script src="https://code.jquery.com/jquery-3.6.0.min.js?v=1"></script>
  </head>

  <body>
    <div class="container">
      <header class="lab-header">
        <div class="logo">
          <div class="lab-name" title="ÊùéÂ≤≥Â∞ä">Yuezun Li</div>
        </div>
        <nav class="nav-menu">
          <a href="index.html">About me</a>
          <a href="publications.html">Publications</a>
          <a href="talks.html">Talks</a>
          <a href="funds.html">Funds</a>
          <a href="services.html">Services</a>
          <a href="lab.html">Lab</a>
        </nav>
        <button class="nav-toggle" aria-label="Toggle Menu">‚ò∞</button>
      </header>

      <div class="col-md-12" style="display: flex; align-items: center">
        <div class="col-md-2" style="margin-right: 20px">
          <img
            style="
              width: 100;
              border-radius: 30%;
              box-shadow: 0 4px 12px rgba(0, 0, 0, 0.25);
            "
            alt="profile photo"
            src="images/people_img/selfie2024.jpg"
          />
          <div
            class="social-icons"
            style="
              margin-top: 15px;
              display: flex;
              align-items: center;
              gap: 15px;
            "
          >
            <a href="mailto:liyuezun@ouc.edu.cn" class="icon-link">
              <i class="fa fa-envelope"></i>
            </a>

            <a
              href="https://scholar.google.com/citations?user=v0Qt7BAAAAAJ&hl=en"
              class="icon-link"
            >
              <i class="fa-solid fa-graduation-cap"></i>
            </a>

            <a href="https://github.com/yuezunli" class="icon-link">
              <i class="fa fa-github"></i>
            </a>
          </div>
        </div>

        <div class="col-md-10" style="float: right">
          <p style="text-align: justify">
            I am an Assistant Professor/Lecturer at
            <a href="https://www.ouc.edu.cn/main.htm"
              >Ocean University of China</a
            >. My research focuses on multimedia forensics and computer vision.
            I have published more than 30 papers in prestigious conferences and
            journals, with
            <b
              >total
              <a
                href="https://scholar.google.com/citations?user=v0Qt7BAAAAAJ&hl=en"
                ><img
                  src="https://img.shields.io/endpoint?url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2Fyuezunli%2Facad-homepage.github.io%40google-scholar-stats%2Fgs_data_shieldsio.json&amp;logo=Google%20Scholar&amp;labelColor=f6f6f6&amp;color=9cf&amp;style=flat&amp;label=citations"
              /></a>
              , including several papers with more than
              <span
                class="low_bound_citations"
                data="v0Qt7BAAAAAJ:_xSYboBqXhAC;v0Qt7BAAAAAJ:35N4QoGY0k4C;v0Qt7BAAAAAJ:_Ybze24A_UAC;v0Qt7BAAAAAJ:g5m5HwL7SMYC"
              ></span>
              citations each</b
            >
            (CVPR20, CVPRW19, ICASSP19, WIFS18). The WIFS18 paper was also
            featured on
            <b>CCTV13‚Äôs "World Weekly" special on deepfakes [7:43]</b>. I serve
            as a reviewer for top conferences and journals, such as TPAMI, TIP,
            IJCV, TIFS, ICCV, CVPR, AAAI, NeurIPS, ICLR, etc. I have been
            recognized on Stanford‚Äôs 2023/2024/2025 list of the top 2% of
            scientists worldwide and am a recipient of the 2024 ACM Qingdao
            Rising Star Award.
            <a href="http://it.ouc.edu.cn/lyz2/main.htm">[Chinese version]</a>
            <!-- <img src="https://img.shields.io/badge/Google Scholar-6400-blue?style=social&logo=google-scholar" style="vertical-align: middle;" /> -->
          </p>

          <p style="text-align: justify">
            I was a Senior Research Scientist at the Department of Computer
            Science and Engineering of
            <a href="https://www.buffalo.edu/">University at Buffalo, SUNY</a>,
            working with
            <a href="https://cse.buffalo.edu/~siweilyu/lyu_lab.html"
              >Siwei Lyu</a
            >. I was a summer intern at
            <a href="https://www.ge.com/research/">GE glocal research center</a>
            during 2016 - 2018. I received Ph.D. degree in computer science at
            <a href="https://www.albany.edu/">University at Albany, SUNY</a> in
            2020, advised by
            <a href="https://cse.buffalo.edu/~siweilyu/lyu_lab.html"
              >Siwei Lyu</a
            >
            (IEEE/IAPR Fellow). I received M.S. degree in Computer Science in
            2015 and B.S. degree in Software Engineering in 2012 at Shandong
            University.
          </p>
        </div>
      </div>

      <div class="col-md-12">
        <p class="highlight-text">
          üì¢ <b>Opening!</b> Currently I am leading the
          <a href="lab.html">VAS lab</a>
          <img src="images/src_img/logo.png" width="25" />. Our lab has multiple
          openings.
          <b
            >Please drop me an email if you are interested in internship / M.S.
            program.</b
          >
          I also advise the
          <a href="https://yuezunli.github.io/planktongroup/">POVA group</a>
          <img src="images/src_img/logo-2.png" width="25" />, which focuses on
          underwater computer vision reserach. Drop me an email if you are
          interested.
        </p>
      </div>

      <div class="col-md-12">
        <h4 class="subhead">üéâ News</h4>
        <ul class="news-list">
          <li
            style="
              position: sticky;
              top: 0;
              background: rgb(255, 255, 255);
              z-index: 10;
            "
          >
            <b
              >[2025.07]: We have released
              <a href="https://github.com/OUC-VAS/Celeb-DF-PP"
                >Celeb-DF++ benchmark</a
              >, an extension of earlier Celeb-DF dataset.</b
            >
            üìå üìå
          </li>
          <li>
            [2025.11]: Three master students are awarded an enterprise
            scholarship (¬•2w). Congratulations! &nbsp;
            <span class="news-new">New!</span>
          </li>
          <li>
            [2025.10]: One paper on domain adaptation is accepted by TIP 2025.
            &nbsp; <span class="news-new">New!</span>
          </li>
          <li>
            [2025.10]: I gave a talk on advances on visual forensics PRCV 2025.
            &nbsp; <span class="news-new">New!</span>
          </li>
          <li>
            [2025.09]: Elected among
            <a
              href="https://topscinet.com/scientist_profile/Li,%20Yuezun/2016/?stype=single_year"
              >World's Top 2% Scientists 2025 by Stanford University</a
            >. &nbsp; <span class="news-new">New!</span>
          </li>
          <li>
            [2025.07]: One paper on proactive deepfake detection is accepted by
            TDSC 2025. &nbsp; <span class="news-new">New!</span>
          </li>
          <li>
            [2025.05]: One paper on underwater stereo matching is accepted by
            TCSVT 2025.
          </li>
          <li>
            [2025.03]: One paper on image manipulation detection is accepted by
            ICME 2025.
          </li>
          <li>
            [2025.02]: Two papers on deepfake detection are accepted by CVPR
            2025.
          </li>
          <li>[2025.02]: The work of TSOM is elected as WACV 2025 Oral.</li>
          <li>
            [2025.01]: I gave a talk on face forensics in the wild at CSIG Young
            Scholars forum. &nbsp;
          </li>
          <li>
            [2024.11]: One paper on phytoplankton tracking is accepted by TCSVT.
            &nbsp;
          </li>
          <li>[2024.11]: Awarded BMVC outstanding reviewer.</li>
          <li>
            [2024.10]: One paper on sequential deepfake detection is accepted by
            WACV.
          </li>
          <li>
            [2024.09]: One paper on deepfake detection is accepted by NeurIPS.
          </li>
          <li>
            [2024.09]: Elected among
            <a
              href="https://topresearcherslist.com/Home/Search?AuthFull=Li,+Yuezun"
              >World's Top 2% Scientists 2024 by Stanford University</a
            >.
          </li>
          <li>
            [2024.09]: One paper on quality-agnostic deepfake detection is
            accepted by ACCV.
          </li>
          <li>
            [2024.08]: One paper on 3D adversarial meshes are accepted by PG.
          </li>
          <li>
            [2024.07]: Two papers on image manipulation detection are accepted
            by BMVC.
          </li>
          <li>
            [2024.07]: One paper on transferable deepfake detection is accepted
            by IEEE TIFS.
          </li>
          <li>
            [2024.07]: One paper on generalizable deepfake detection is accepted
            by ECCV.
          </li>
          <li>
            [2024.05]: One paper on multi-face deepfake detection is accepted by
            IEEE TCSVT.
          </li>
          <li>
            [2024.04]: One paper on deepfake detection is accepted by IEEE TIFS.
            &nbsp;
          </li>
          <li>
            [2024.04]: One paper on deepfake defense is accepted by IEEE TETC.
            &nbsp;
          </li>
          <li>
            [2024.04]: One paper on image-text retrieval is accepted by ICMR.
            &nbsp;
          </li>
          <li>
            [2024.02]: One paper on image-text retrieval is accepted by KBS.
            &nbsp;
          </li>
          <li>
            [2024.01]: One paper on deepfake defense is accepted by CVIU. &nbsp;
          </li>
          <li>
            [2023.12]: One paper on adversarial robustness is accepted by
            ICASSP. &nbsp;
          </li>
          <li>
            [2023.11]: I gave a talk at a forum in CSIG ChinaFMS conference
            2023. &nbsp;
          </li>
          <li>
            [2023.10]: One paper on adversarial robustness is accepted by CVIU.
            &nbsp;
          </li>
          <li>
            [2023.07]: I gave a talk at a forum in
            <a
              href="https://conf.ccf.org.cn/web/html5/index.html?globalId=m1082232101379641344167806583986&type=1"
              >CCF CFTC 2023</a
            >
            on Recent Advances on CV Security. &nbsp;
          </li>
          <li>
            [2023.06]: One ICME paper entitled Forensics Forest is selected as
            Oral. &nbsp;
          </li>
          <li>
            [2023.05]: A dataset on phytoplankton observation is accepted by
            JMSE. &nbsp;
          </li>
          <li>
            [2023.05]: One paper on Semantic Segmentation is accepted by IEEE
            TNNLS.
          </li>
          <li>
            [2023.04]: One paper on Universal Domain Adaptation is accepted by
            IEEE TCSVT.
          </li>
          <li>
            [2023.03]: Two papers on DeepFake defense are accepted by ICME.
          </li>
          <li>
            [2023.02]: One paper on Transferable Adversarial Attack is accepted
            by COSE.
          </li>
          <li>
            [2022.12]: I gave a talk at CCBR 2022 on Recent Advances on AI
            Security.
          </li>
          <li>
            [2022.11]: One paper on DeepFake detection is accepted by Pattern
            Recognition.
          </li>
          <li>
            [2022.08]: I gave a tutorial at
            <a href="http://ieee-mipr.org/Tutorials.html"
              >MIPR 2022 on DeepFake Creation, Detection and Obstruction</a
            >.
          </li>
          <li>
            [2022.07]: I gave a tutorial at
            <a href="https://2022.ieeeicme.org/tutorials.html"
              >ICME 2022 on DeepFake creation and detection</a
            >.
          </li>
          <li>[2022.07]: One paper on face synthesis is accepted by PRL.</li>
          <li>[2022.06]: One paper on DeepFake defense is accepted by ICIP.</li>
          <li>
            [2022.06]: One paper on DeepFake detection is accepted by Pattern
            Recognition.
          </li>
          <li>
            [2022.05]: One paper on DeepFake detection is accepted by MIPR.
          </li>
          <li>
            [2022.02]: One book chapter is publised on Handbook of Digital Face
            Manipulation and Detection, Springer.
          </li>
          <li>
            [2021.12]: One book chapter is publised on Deep Learning-Based Face
            Analytics, 2021. Springer. &nbsp;
          </li>
          <li>
            [2021.10]: One paper on adversarial attack is accepted by CVIU 2021.
            &nbsp;
          </li>
          <li>
            [2021.08]: One paper on scene parsing is accepted by TNNLS 2021.
            &nbsp;
          </li>
          <li>
            [2021.08]: I gave a tutorial at IJCB 2021 on Trustworthy Biometrics.
            &nbsp;
          </li>
          <li>
            [2021.07]: One paper on backdoor attack is accepted by ICCV 2021.
            &nbsp;
          </li>
          <li>
            [2021.06]: With my PhD advisor Prof. Siwei Lyu, I gave a tutorial at
            ICASSP 2021 on DeepFake Generation and Detection [<a
              href="https://youtu.be/_AQfHYaQC6o"
              >Part 1</a
            >] [<a href="">Part 2</a>] [<a href="https://youtu.be/IdW37D5JYg4"
              >Part 3</a
            >].
          </li>
          <li>
            [2021.06]: We are releasing DeepFake Game Competition (DFGC) dataset
            [<a href="https://github.com/yuezunli/celeb-deepfakeforensics"
              >Here</a
            >]
          </li>
          <li>
            [2021.05]: We are holding DeepFake Game Competition (DFGC) [<a
              href="http://dfgc2021.iapr-tc4.org/"
              >Here</a
            >]
          </li>
          <li>
            [2021.03]: One paper on DeepFake-o-meter is accepted by SADFE in
            conjunction with the IEEE S&P 2021
          </li>
          <li>[2021.02]: One paper is accepted by ICASSP 2021.</li>
          <li>[2020.02]: Our paper on Celeb-DF is accepted by CVPR 2020.</li>
        </ul>
      </div>

      <div class="col-md-12">
        <h4 class="subhead">
          üìù Selected publications
          <font size="1"
            >(+ Advised student, ‚úâÔ∏è Corresponding author, * Equal
            contribution)</font
          >
        </h4>
        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/hypersfda2024arxiv.png" />
          </div>
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/abs/2405.06916">
                <papertitle
                  >HG-SFDA: HyperGraph Learning Meets Source-free Unsupervised
                  Domain Adaptation</papertitle
                >
              </a>
              <br />
              Jinkun Jiang<sup>+</sup>, Qingxuan Lv, <b>Yuezun Li</b
              ><sup>‚úâÔ∏è</sup>, Yong Du, Junyu Dong<sup>‚úâÔ∏è</sup>, Sheng Chen, Hui
              Yu
              <br />
              IEEE Transactions on Image Processing (<b>TIP</b>), 2025.
              <br />
              <a href="https://github.com/OUC-POVA/HG-SFDA">[Code]</a>
              <b>[CCF A]</b>
              <b>
                <span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:fEOibwPWpKIC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <!-- FacePoison -->
        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/faceposion2024arxiv.png" />
          </div>
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/pdf/2412.01101">
                <papertitle
                  >Hiding Faces in Plain Sight: Defending DeepFakes by
                  Disrupting Face Detection</papertitle
                >
              </a>
              <br />
              Delong Zhu<sup>+</sup>, <b>Yuezun Li</b><sup>‚úâÔ∏è</sup>, Baoyuan Wu,
              Jiaran Zhou, Zhibo Wang, Siwei Lyu
              <br />
              IEEE Transactions On Dependable And Secure Computing
              (<b>TDSC</b>), 2025. (Extended from ICME 2023)
              <br />
              <a href="https://github.com/OUC-VAS/FacePoison">[Code]</a>
              <b>[CCF A]</b>
              <b>
                <span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:UHK10RUVsp4C"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <!-- Forensics Adapter -->
        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/forensicsadapter2025cvpr.png" />
          </div>
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/pdf/2411.19715v2">
                <papertitle
                  >Forensics Adapter: Adapting CLIP for Generalizable Face
                  Forgery Detection</papertitle
                >
              </a>
              <br />
              Xinjie Cui<sup>+</sup>, <b>Yuezun Li</b><sup>‚úâÔ∏è</sup>, Ao Luo,
              Jiaran Zhou, Junyu Dong
              <br />
              IEEE Conference on Computer Vision and Pattern Recognition
              (<b>CVPR</b>), 2025.
              <br />
              <a href="https://github.com/OUC-VAS/ForensicsAdapter">[Code]</a>
              <b>[CCF A]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:UHK10RUVsp4C"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <!-- Devil Hides -->
        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/devil2025cvpr.png" />
          </div>
          <div class="paper-info">
            <p>
              <a href="">
                <papertitle
                  >Where the Devil Hides: Deepfake Detectors Can No Longer Be
                  Trusted</papertitle
                >
              </a>
              <br />
              Shuaiwei Yuan<sup>+</sup>, Junyu Dong, <b>Yuezun Li</b
              ><sup>‚úâÔ∏è</sup>
              <br />
              IEEE Conference on Computer Vision and Pattern Recognition
              (<b>CVPR</b>), 2025.
            </p>
            <b>[CCF A]</b>
            <b
              ><span
                class="show_paper_citations"
                data="v0Qt7BAAAAAJ:_Re3VWB3Y0AC"
              ></span
            ></b>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/uwstereo2024arxiv.png" />
          </div>
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/abs/2409.01782">
                <papertitle
                  >UWStereo: A Large Synthetic Dataset for Underwater Stereo
                  Matching</papertitle
                >
              </a>
              <br />
              Qingxuan Lv<sup>+</sup>, Junyu Dong<sup>‚úâÔ∏è</sup>, <b>Yuezun Li</b
              ><sup>‚úâÔ∏è</sup>, Sheng Chen, Hui Yu, Shu Zhang, Wenhan Wang
              <br />
              IEEE Transactions on Circuits and Systems for Video Technology
              (<b>TCSVT</b>), 2025.
              <br />
              <a href="https://github.com/lqxisok/UWStereo">[Code]</a>
              <b>[CCF B]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:5awf1xo2G04C"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/tsom2024wacv.png" />
          </div>
          <div class="paper-info">
            <p>
              <a
                href="https://openaccess.thecvf.com/content/WACV2025/papers/Li_Texture_Shape_and_Order_Matter_A_New_Transformer_Design_for_WACV_2025_paper.pdf"
              >
                <papertitle
                  >Texture, Shape and Order Matter: A New Transformer Design for
                  Sequential DeepFake Detection</papertitle
                >
              </a>
              <br />
              Yunfei Li<sup>+</sup>, <b>Yuezun Li</b><sup>‚úâÔ∏è</sup>, Xin Wang,
              Baoyuan Wu, Jiaran Zhou, Junyu Dong
              <br />
              IEEE/CVF Winter Conference on Applications of Computer Vision
              (<b>WACV</b>), 2025.
              <b><span style="color: red">(Oral)</span></b>
              <br />
              <a href="https://github.com/OUC-VAS/TSOM">[Code]</a>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:Y5dfb0dijaUC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/freqblender2024neurips.png" />
          </div>
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/abs/2404.13872">
                <papertitle
                  >FreqBlender: Enhancing DeepFake Detection by Blending
                  Frequency Knowledge</papertitle
                > </a
              ><br />
              Hanzhe Li<sup>+</sup>, Jiaran Zhou, <b>Yuezun Li</b><sup>‚úâÔ∏è</sup>,
              Baoyuan Wu, Bin Li, Junyu Dong<br />
              Annual Conference on Neural Information Processing Systems
              (<b>NeurIPS</b>), 2024.<br />
              <a href="https://github.com/OUC-VAS/FreqBlender">[Code]</a>
              <b>[CCF A]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:W5xh706n7nkC"
                ></span
              ></b>
            </p>
          </div>
        </div>
        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/domainforensics2024tifs.png" />
          </div>
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/abs/2312.10680">
                <papertitle
                  >DomainForensics: Exposing Face Forgery across Domains via
                  Bi-directional Adaptation</papertitle
                > </a
              ><br />
              Qingxuan Lv<sup>+</sup>, <b>Yuezun Li</b><sup>‚úâÔ∏è</sup>, Junyu
              Dong<sup>‚úâÔ∏è</sup>, Sheng Chen, Hui Yu, Huiyu Zhou, Shu Zhang<br />
              IEEE Transactions on Information Forensics and Security
              (<b>TIFS</b>), 2024.<br />
              <a href="https://github.com/OUC-VAS/DomainForensics">[Code]</a>
              <b>[CCF A]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:XiVPGOgt02cC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/forest2023icme.png" />
          </div>
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/abs/2308.00964">
                <papertitle
                  >ForensicsForest Family: Multi-scale Hierarchical Cascade
                  Forests for GAN Faces</papertitle
                > </a
              ><br />
              Jiucui Lu<sup>+</sup>, Jiaran Zhou<sup>‚úâÔ∏è</sup>, Junyu Dong, Bin
              Li, Siwei Lyu, <b>Yuezun Li</b><sup>‚úâÔ∏è</sup><br />
              IEEE Transactions on Information Forensics and Security
              (<b>TIFS</b>), 2024. (Extended from ICME 2023)<br />
              <a href="https://github.com/OUC-VAS/ForensicsForest">[Code]</a>
              <b>[CCF A]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:1qzjygNMrQYC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/multi-face2023arxiv.png" />
          </div>
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/abs/2308.01520">
                <papertitle
                  >COMICS: End-to-end Bi-grained Contrastive Learning for
                  Multi-face Forgery Detection</papertitle
                > </a
              ><br />
              Cong Zhang<sup>+</sup>, Honggang Qi, Shuhui Wang, <b>Yuezun Li</b
              ><sup>‚úâÔ∏è</sup>, Siwei Lyu<br />
              IEEE Transactions on Circuits and Systems for Video Technology
              (<b>TCSVT</b>), 2024.<br />
              <a href="https://github.com/zhangconghhh/COMICS">[Code]</a>
              <b>[CCF B]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:9vf0nzSNQJEC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/pytracker2024arxiv.png" />
          </div>
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/abs/2407.00352">
                <papertitle
                  >PhyTracker: An Online Tracker for Phytoplankton</papertitle
                > </a
              ><br />
              Yang Yu<sup>+</sup>, Qingxuan Lv, <b>Yuezun Li</b><sup>‚úâÔ∏è</sup>,
              Zhiqiang Wei, Junyu Dong<sup>‚úâÔ∏è</sup><br />
              IEEE Transactions on Circuits and Systems for Video Technology
              (<b>TCSVT</b>), 2024.<br />
              <a href="https://github.com/OUC-PVOA/PhyTracker">[Code]</a>
              <b>[CCF B]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:LjlpjdlvIbIC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/lafea2023tcsvt.png" width="120" />
          </div>
          <div class="paper-info">
            <p>
              <a href="">
                <papertitle>
                  LaFea: Learning Latent Representation beyond Feature for
                  Universal Domain Adaptation
                </papertitle>
              </a>
              <br />
              Qingxuan Lv<sup>+</sup>, <b>Yuezun Li</b><sup>‚úâÔ∏è</sup>, Junyu
              Dong<sup>‚úâÔ∏è</sup>, Ziqian Guo.
              <br />
              IEEE Transactions on Circuits and Systems for Video Technology
              (<b>TCSVT</b>), 2023.
              <br />
              <b>[CCF B]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:mvPsJ3kp5DgC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/big2022pr.png" width="180" />
          </div>
          <div class="paper-info">
            <p>
              <a href="">
                <papertitle>
                  Watching the BiG Artifacts: Exposing DeepFake Videos via
                  Bi-Granularity Artifacts
                </papertitle>
              </a>
              <br />
              Han Chen<sup>*</sup>, <b>Yuezun Li</b><sup>*</sup>, Dongdong Lin,
              Bin Li, Junqiang Wu.
              <br />
              Pattern Recognition (<b>PR</b>), 2023.
              <br />
              <b>[CCF B]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:vRqMK49ujn8C"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/ssba2021iccv.png" width="180" />
          </div>
          <div class="paper-info">
            <p>
              <a href="">
                <papertitle
                  >Invisible Backdoor Attack with Sample-Specific
                  Triggers</papertitle
                > </a
              ><br />
              <b>Yuezun Li</b>, Yiming Li, Baoyuan Wu, Longkang Li, Ran He,
              Siwei Lyu.<br />
              IEEE International Conference on Computer Vision (<b>ICCV</b>),
              2021.<br />
              <a href="https://github.com/yuezunli/ISSBA">[Code]</a>
              <b>[CCF A]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:p2g8aNsByqUC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img src="images/paper_img/celebdf2020cvpr.png" width="150" />
          </div>
          <div class="paper-info">
            <p>
              <a href=""
                ><papertitle
                  >Celeb-DF: A Large-scale Challenging Dataset for DeepFake
                  Forensics</papertitle
                ></a
              ><br />
              <b>Yuezun Li</b>, Xin Yang, Pu Sun, Honggang Qi, Siwei Lyu.<br />
              IEEE Conference on Computer Vision and Pattern Recognition
              (<b>CVPR</b>), 2020.<br />
              <a href="https://forms.gle/QpoPFy6T9vHmPUQdA"
                >[Dataset download]</a
              >
              <b>[CCF A]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:_xSYboBqXhAC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img
              src="images/paper_img/fwa.png"
              onerror="this.style.display='none';"
            />
          </div>
          <div class="paper-info">
            <p>
              <a href=""
                ><papertitle
                  >Exposing DeepFake Videos By Detecting Face Warping
                  Artifacts</papertitle
                ></a
              ><br />
              <b>Yuezun Li</b>, Siwei Lyu.<br />
              IEEE Conference on Computer Vision and Pattern Recognition
              (<b>CVPR</b>) Workshop on Media Forensics, 2019.<br />
              <a href="https://github.com/danmohaha/CVPRW2019_Face_Artifacts"
                >[Code]</a
              >
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:35N4QoGY0k4C"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img
              src="images/paper_img/headpose.png"
              onerror="this.style.display='none';"
            />
          </div>
          <div class="paper-info">
            <p>
              <a href=""
                ><papertitle
                  >Exposing Deep Fakes Using Inconsistent Head Poses</papertitle
                ></a
              ><br />
              Xin Yang<sup>*</sup>, <b>Yuezun Li</b><sup>*</sup>, Siwei Lyu.<br />
              IEEE International Conference on Acoustics, Speech, and Signal
              Processing (<b>ICASSP</b>), 2019<br />
              <a
                href="https://bitbucket.org/ericyang3721/headpose_forensic/src/master/"
                >[Code]</a
              >
              <b>[CCF B]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:_Ybze24A_UAC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img
              src="images/paper_img/lloc2019ihmmsec.png"
              onerror="this.style.display='none';"
            />
          </div>
          <div class="paper-info">
            <p>
              <a href=""
                ><papertitle
                  >Exposing GAN-synthesized Faces Using Landmark
                  Locations</papertitle
                ></a
              ><br />
              Xin Yang, <b>Yuezun Li</b>, Honggang Qi, Siwei Lyu.<br />
              ACM Workshop on Information Hiding and Multimedia Security
              (<b>IHMMSec</b>), 2019<br />
              <b>[CCF C]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:SeFeTyx0c_EC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img
              src="images/paper_img/ssm2019bmvc.png"
              onerror="this.style.display='none';"
            />
          </div>
          <div class="paper-info">
            <p>
              <a href="">
                <papertitle
                  >Exploring the Vulnerability of Single Shot Module in Object
                  Detectors via Imperceptible Background Patches</papertitle
                > </a
              ><br />
              <b>Yuezun Li</b>, Xiao Bian, Ming-Ching Chang, Siwei Lyu.<br />
              British Machine Vision Conference (<b>BMVC</b>), 2019.<br />
              <b>[CCF C]</b>
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:a0OBvERweLwC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img
              src="images/paper_img/wifs18.png"
              onerror="this.style.display='none';"
            />
          </div>
          <div class="paper-info">
            <p>
              <a href=""
                ><papertitle
                  >In Ictu Oculi: Exposing AI Generated Fake Face Videos by
                  Detecting Eye Blinking</papertitle
                ></a
              ><br />
              <b>Yuezun Li</b>, Ming-Ching Chang, Siwei Lyu.<br />
              IEEE International Workshop on Information Forensics and Security
              (<b>WIFS</b>), 2018<br />
              <a href="https://github.com/danmohaha/WIFS2018_In_Ictu_Oculi"
                >[Code]</a
              >
              /
              <a href="https://forms.gle/fx6m5PXug99MqJRL9"
                >[Dataset download]</a
              ><b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:g5m5HwL7SMYC"
                ></span
              ></b>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <div class="paper-image">
            <img
              src="images/paper_img/RAP2018bmvc.png"
              onerror="this.style.display='none';"
            />
          </div>
          <div class="paper-info">
            <p>
              <a href=""
                ><papertitle
                  >Robust Adversarial Perturbation on Deep Proposal-based
                  Models</papertitle
                ></a
              ><br />
              <b>Yuezun Li</b>, Daniel Tian, Ming-Ching Chang, Xiao Bian, Siwei
              Lyu.<br />
              British Machine Vision Conference (<b>BMVC</b>), 2018<br />
              <a href="https://github.com/yuezunli/BMVC2018R-AP">[Code]</a>
              <b>[CCF C]</b
              ><b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:yD5IFk8b50cC"
                ></span
              ></b>
            </p>
          </div>
        </div>
        <!-- </div> -->
      </div>

      <div class="col-md-12">
        <!-- <div class="paper-container"></div> -->
        <h4 class="subhead">üîî Preprints</h4>
        <div class="paper-item">
          <!-- Left Image -->
          <div class="paper-image">
            <img src="images/paper_img/celeb-df++.png" />
          </div>
          <!-- Right text -->
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/abs/2507.18015">
                <papertitle>
                  Celeb-DF++: A Large-scale Challenging Video DeepFake Benchmark
                  for Generalizable Forensics
                </papertitle>
              </a>
              <br />
              <b>Yuezun Li</b><sup>‚úâÔ∏è</sup>, Delong Zhu<sup>+</sup>, Xinjie
              Cui<sup>+</sup>, Siwei Lyu
              <br />
              arXiv:2507.18015, 2025. (An extension of Celeb-DF dataset)
              <br />
              <a href="https://github.com/OUC-VAS/Celeb-DF-PP">[Code]</a>
            </p>
          </div>
        </div>

        <div class="paper-item">
          <!-- Left Image -->
          <div class="paper-image">
            <img src="images/paper_img/TSOM++2025arxiv.png" />
          </div>
          <!-- Right text -->
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/pdf/2404.13873">
                <papertitle>
                  Texture, Shape, Order, and Relation Matter: A New Transformer
                  Design for Sequential DeepFake Detection
                </papertitle>
              </a>
              <br />
              Yunfei Li<sup>+</sup>, <b>Yuezun Li</b><sup>‚úâÔ∏è</sup>, Baoyuan Wu,
              Junyu Dong, Guopu Zhu, Siwei Lyu <br />arXiv:2404.13873, 2025.
              (Extended from WACV 2025 (oral))
              <br />
              <a href="https://github.com/OUC-VAS/TSOM">[Code]</a>
              <br />
            </p>
          </div>
        </div>

        <div class="paper-item">
          <!-- Left Image -->
          <div class="paper-image">
            <img src="images/paper_img/forensicsadapter++2025arxiv.png" />
          </div>

          <!-- Right text -->
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/pdf/2411.19715">
                <papertitle>
                  Forensics Adapter: Unleashing CLIP for Generalizable Face
                  Forgery Detection
                </papertitle>
              </a>
              <br />
              Xinjie Cui<sup>+</sup>, <b>Yuezun Li</b><sup>‚úâÔ∏è</sup>, Delong Zhu,
              Jiaran Zhou, Junyu Dong, Siwei Lyu
              <br />
              arXiv:2411.19715, 2025. (Extended from CVPR 2025)
              <br />
              <a href="https://github.com/OUC-VAS/ForensicsAdapter">[Code]</a>
              <br />
            </p>
          </div>
        </div>

        <div class="paper-item">
          <!-- Left Image -->
          <div class="paper-image">
            <img src="images/paper_img/dfcam2024arxiv.png" />
          </div>

          <!-- Right text -->
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/pdf/2409.03200">
                <papertitle> Active Fake: DeepFake Camouflage </papertitle>
              </a>
              <br />
              Pu Sun<sup>+</sup>, Honggang Qi<sup>‚úâÔ∏è</sup>, <b>Yuezun Li</b
              ><sup>‚úâÔ∏è</sup>
              <br />
              arXiv:2409.03200, 2024.
            </p>
          </div>
        </div>

        <div class="paper-item">
          <!-- Left Image -->
          <div class="paper-image">
            <img src="images/paper_img/hiding2019arxiv.png" />
          </div>

          <!-- Right text -->
          <div class="paper-info">
            <p>
              <a href="https://arxiv.org/pdf/1906.09288">
                <papertitle>
                  Hiding Faces in Plain Sight: Disrupting AI Face Synthesis with
                  Adversarial Perturbations
                </papertitle>
              </a>
              <br />
              <b>Yuezun Li</b>, Xin Yang, Baoyuan Wu, Siwei Lyu
              <br />
              arXiv:1906.09288, 2019.<br />
              <b
                ><span
                  class="show_paper_citations"
                  data="v0Qt7BAAAAAJ:ZHo1McVdvXMC"
                ></span
              ></b>
              (preliminary version of TDSC 2025)
            </p>
          </div>
        </div>
        <!-- </div> -->
      </div>
    </div>
    <hr />

    <div class="container-fluid">
      <p style="text-align: center">
        <a href="https://clustrmaps.com/site/1bcyj" title="Visit tracker"
          ><img
            src="//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=m&d=OuDVjBphuIV15vb_K9-k4Nnidvdol9qiwaJ6-fy4sXQ"
        /></a>
      </p>
      <!-- <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=-yCBFYKb96-8yFOkGerGi6ZdI4cxXiZdFXRKfDr9owM"></script> -->
      <p style="text-align: center">&copy; VAS-Lab</p>
    </div>

    <!-- Javascript files -->
    <script src="js/jquery-1.9.1.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script>
      $("#main-carousel").carousel();
    </script>

    <!-- <script>
        const images = ["src_img/earth.gif", "src_img/ai1.gif", "src_img/ai2.gif", "src_img/ai3.gif", "src_img/ai4.gif"]; // 7Âº†ÂõæÁâá
        const date = new Date();
        const index = date.getDate() % images.length; // ‰ª•Êó•ÊúüËÆ°ÁÆóÁ¥¢Âºï
        document.getElementById("dailyImage").src = images[index];
    </script> -->

    <script>
      document.querySelectorAll(".nav-menu a").forEach((link) => {
        if (link.href === window.location.href) {
          link.classList.add("active");
        }
      });
    </script>

    <script>
      $(document).ready(function () {
        $.getJSON(
          "https://cdn.jsdelivr.net/gh/yuezunli/acad-homepage.github.io@google-scholar-stats/gs_data.json",
          function (data) {
            var citationEles = document.getElementsByClassName(
              "show_paper_citations"
            );
            Array.prototype.forEach.call(citationEles, (element) => {
              var paperId = element.getAttribute("data");
              try {
                var numCitations =
                  data["publications"][paperId]["num_citations"];
              } catch (err) {
                console.error("Error:", err);
                return; // Áõ¥Êé•ÈÄÄÂá∫ÂΩìÂâçÂáΩÊï∞
              }
              element.innerHTML = "[üéì Citations: " + numCitations + "]";
              if (numCitations > 100) {
                element.style.color = "red";
                // element.style.fontWeight = 'bold';
              }
            });
          }
        );
      });
    </script>

    <script>
      const toggleBtn = document.querySelector(".nav-toggle");
      const navMenu = document.querySelector(".nav-menu");
      toggleBtn.addEventListener("click", () => {
        navMenu.classList.toggle("show");
      });
    </script>
  </body>
</html>
